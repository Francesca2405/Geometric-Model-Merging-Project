{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Configurazione Device e Riproducibilità\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device in uso: {device}\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKBD125oiM3L",
        "outputId": "5b6d84d7-0c7f-4454-bd8b-198d3fa08b54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in uso: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caricamento Dati MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST('./', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "# Definizione Modello (SimpleMLP)\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=512, output_dim=10):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ujn7csZ2idoh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_model(model_id, data_loader, num_epochs=3):\n",
        "    #Addestra un modello con un seed specifico e lo salva\n",
        "    # Seed diverso per ogni modello = inizializzazione diversa\n",
        "    torch.manual_seed(42 + model_id)\n",
        "\n",
        "    model = SimpleMLP().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Addestramento\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    path = f'model_checkpoint_{model_id}.pth'\n",
        "    torch.save(model.state_dict(), path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "ZbCasUZbixih"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_models(reference_state_dict, target_state_dict):\n",
        "\n",
        "  # Allinea i neuroni del target al reference risolvendo la Permutation Invariance.\n",
        "\n",
        "    new_target_dict = target_state_dict.copy()\n",
        "\n",
        "    # Lavoriamo sullo stesso device\n",
        "    ref_w = reference_state_dict['layer1.weight'].to(device)\n",
        "    tgt_w = target_state_dict['layer1.weight'].to(device)\n",
        "\n",
        "    # Calcolo Similarità (Prodotto Scalare)\n",
        "    similarity = torch.matmul(ref_w, tgt_w.T)\n",
        "\n",
        "    # Trova la permutazione ottimale (Argmax Greedy)\n",
        "    # Per ogni neurone del ref, troviamo il neurone del target più simile\n",
        "    permutation = torch.argmax(similarity, dim=1)\n",
        "\n",
        "    # Applica la permutazione ai pesi\n",
        "    # Layer 1 (Output): Riordiniamo le righe\n",
        "    new_target_dict['layer1.weight'] = target_state_dict['layer1.weight'][permutation]\n",
        "    new_target_dict['layer1.bias'] = target_state_dict['layer1.bias'][permutation]\n",
        "\n",
        "    # Layer 2 (Input): Riordiniamo le colonne\n",
        "    new_target_dict['layer2.weight'] = target_state_dict['layer2.weight'][:, permutation]\n",
        "\n",
        "    return new_target_dict"
      ],
      "metadata": {
        "id": "oNfOla_OjJ-G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_align_and_vectorize(N, device):\n",
        "\n",
        "    # Carica -> Allinea al Modello 1 -> Vettorizza -> Normalizza.\n",
        "\n",
        "    aligned_vectors = []\n",
        "\n",
        "    # Carichiamo il Modello 1 come Riferimento\n",
        "    path_ref = 'model_checkpoint_1.pth'\n",
        "    if not os.path.exists(path_ref): return None\n",
        "    ref_state_dict = torch.load(path_ref, map_location=device)\n",
        "\n",
        "    # Chiavi per appiattire i pesi in ordine\n",
        "    keys = list(ref_state_dict.keys())\n",
        "\n",
        "    for i in range(1, N + 1):\n",
        "        path = f'model_checkpoint_{i}.pth'\n",
        "        curr_state_dict = torch.load(path, map_location=device)\n",
        "\n",
        "        # FASE DI ALLINEAMENTO\n",
        "        if i > 1:\n",
        "\n",
        "            final_dict = align_models(ref_state_dict, curr_state_dict)\n",
        "        else:\n",
        "            final_dict = curr_state_dict # Il modello 1 è già \"allineato a se stesso\"\n",
        "\n",
        "        # Vettorizzazione\n",
        "        tensors = [final_dict[k].view(-1) for k in keys]\n",
        "        v_flat = torch.cat(tensors)\n",
        "\n",
        "        # Normalizzazione Sferica (L2)\n",
        "        v_norm = F.normalize(v_flat, p=2, dim=0)\n",
        "        aligned_vectors.append(v_norm)\n",
        "\n",
        "    return torch.stack(aligned_vectors)"
      ],
      "metadata": {
        "id": "yI_ztL3TjrtR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spherical_loss(merged_vector, original_vectors, weights):\n",
        "    # Prodotto scalare e clamp per stabilità numerica\n",
        "    dot = torch.einsum('d, nd -> n', merged_vector, original_vectors)\n",
        "    dot = torch.clamp(dot, -1.0 + 1e-7, 1.0 - 1e-7)\n",
        "    # Distanza geodetica\n",
        "    dist = torch.acos(dot)\n",
        "    return 0.5 * torch.sum(weights * (dist ** 2))\n",
        "\n",
        "def find_karcher_mean(vectors, weights, lr=0.5, max_iter=200):\n",
        "    # Inizializzazione: Media Euclidea proiettata\n",
        "    mean_init = torch.mean(vectors, dim=0)\n",
        "    m = F.normalize(mean_init, p=2, dim=0).clone().detach().requires_grad_(True)\n",
        "\n",
        "    optimizer = optim.SGD([m], lr=lr)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        optimizer.zero_grad()\n",
        "        loss = spherical_loss(m, vectors, weights)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Proietta di nuovo sulla sfera dopo il passo\n",
        "        with torch.no_grad():\n",
        "            m.data = F.normalize(m.data, p=2, dim=0)\n",
        "\n",
        "    return m.detach()"
      ],
      "metadata": {
        "id": "EQ-GE2dCkMxf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_vectorize(vector, ref_model): # Dal vettore piatto torna al state_dict\n",
        "    state_dict = ref_model.state_dict()\n",
        "    new_dict = {}\n",
        "    idx = 0\n",
        "    for k, v in state_dict.items():\n",
        "        num = v.numel()\n",
        "        new_dict[k] = vector[idx : idx+num].view(v.shape)\n",
        "        idx += num\n",
        "    return new_dict\n",
        "\n",
        "def evaluate(weights):\n",
        "    model = SimpleMLP().to(device)\n",
        "    model.load_state_dict(weights)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += len(data)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "c0AH_dxLkT5_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista degli esperimenti\n",
        "N_values = [3, 5, 10]\n",
        "results_table = []\n",
        "\n",
        "print(\"INIZIO ESPERIMENTI\")\n",
        "\n",
        "for N in N_values:\n",
        "    print(f\"Processing N={N}\")\n",
        "\n",
        "    # Training\n",
        "    for i in range(1, N + 1):\n",
        "        if not os.path.exists(f'model_checkpoint_{i}.pth'):\n",
        "            train_and_save_model(i, train_loader)\n",
        "\n",
        "    # Caricamento, Allineamento e Vettorizzazione\n",
        "    V = load_align_and_vectorize(N, device)\n",
        "    weights = torch.ones(N).to(device) / N # Pesi uniformi\n",
        "\n",
        "    # Calcolo Karcher Mean\n",
        "    karcher_vec = find_karcher_mean(V, weights)\n",
        "\n",
        "    # Calcolo Media Euclidea\n",
        "    euclidean_vec = torch.mean(V, dim=0)\n",
        "\n",
        "    # Valutazione\n",
        "    ref_model = SimpleMLP().to('cpu')\n",
        "\n",
        "    # Eval Karcher\n",
        "    w_karcher = inverse_vectorize(karcher_vec.cpu(), ref_model)\n",
        "    acc_karcher = evaluate(w_karcher)\n",
        "\n",
        "    # Eval Euclidean\n",
        "    w_euclid = inverse_vectorize(euclidean_vec.cpu(), ref_model)\n",
        "    acc_euclid = evaluate(w_euclid)\n",
        "\n",
        "    # Save results\n",
        "    gap = acc_karcher - acc_euclid\n",
        "    results_table.append((N, acc_karcher, acc_euclid, gap))\n",
        "    print(f\"Karcher: {acc_karcher:.4f} | Euclidean: {acc_euclid:.4f}\")\n",
        "\n",
        "# --- STAMPA TABELLA FINALE ---\n",
        "print(\"FINAL RESULTS TABLE\")\n",
        "print(\"|  N  | Karcher Acc | Euclidean Acc |   Gap   |\")\n",
        "print(\"|-----|-------------|---------------|---------|\")\n",
        "for row in results_table:\n",
        "    print(f\"| {row[0]:<3} |   {row[1]*100:.2f}%    |    {row[2]*100:.2f}%     | {row[3]*100:+.2f}%  |\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOMXaneuks8m",
        "outputId": "43b6d105-791c-437b-bc4c-4d3626f45c19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INIZIO ESPERIMENTI\n",
            "Processing N=3\n",
            "Karcher: 0.9672 | Euclidean: 0.9665\n",
            "Processing N=5\n",
            "Karcher: 0.9632 | Euclidean: 0.9614\n",
            "Processing N=10\n",
            "Karcher: 0.9564 | Euclidean: 0.9543\n",
            "Processing N=20\n",
            "Karcher: 0.9603 | Euclidean: 0.9596\n",
            "FINAL RESULTS TABLE\n",
            "|  N  | Karcher Acc | Euclidean Acc |   Gap   |\n",
            "|-----|-------------|---------------|---------|\n",
            "| 3   |   96.72%    |    96.65%     | +0.07%  |\n",
            "| 5   |   96.32%    |    96.14%     | +0.18%  |\n",
            "| 10  |   95.64%    |    95.43%     | +0.21%  |\n",
            "| 20  |   96.03%    |    95.96%     | +0.07%  |\n"
          ]
        }
      ]
    }
  ]
}